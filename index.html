
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Shashank is a Masters student with Prof. Dan Roth. His research interests are in Machine Learning and NLP.">
    <meta name="keywords" content="Shashank Gupta, Machine Learning, NLP, Deep Learning, Graduate student, Computer Science, Dan Roth">
    <link rel="icon" href="images/cogcomp.png">

    <title>Shashank Gupta</title>

    <!-- Bootstrap core CSS -->
    <link href="http://getbootstrap.com/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="http://getbootstrap.com/assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!--<script src="../../assets/js/ie-emulation-modes-warning.js"></script>-->
    <link rel="stylesheet" type="text/css" href="style.css"/>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-57880796-2', 'auto');
  ga('send', 'pageview');
</script>
  </head>

  <body>

    <div class="container">

      <!-- Static navbar -->
      <nav class="navbar navbar-default">
        <div class="container-fluid">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#">Shashank Gupta</a>
          </div>
          <div id="navbar" class="navbar-collapse collapse">
            <ul class="nav navbar-nav">
              <li class="active"><a href="#">Home</a></li>
              <li><a href="#pubs">Publications</a></li>
              <li><a href="#misc">Miscellaneous</a></li>
              <li><a href="#contact">Contact</a></li>
            </ul>
          </div>
        </div>
      </nav>

        <div class="well">
        <table style="border-spacing: 10px;border-collapse: separate;">
          <tbody>
            <tr>
              <td width="0pct" valign="top">
                <div><span class="lfloat"><img width="120px" src="images/Gupta_Shashank.jpg" alt="" title="Shashank Gupta"></span></div>
              </td>
              <td valign="top" margin-top="10px" style="font-size: 0pt;">
              <p>
              <span style="font-size: 20px">I'm a Masters student in the Computer Science Department at the
                <a href="https://cs.illinois.edu/">University of Illinois at Urbana-Champaign</a>, where I'm working
                with Prof. <a href="http://www.cis.upenn.edu/~danroth/">Dan Roth</a> in his amazing
                <a href="http://cogcomp.org/">CogComp</a> group! Before that, I had the pleasure of working with
                Prof. <a href="http://people.mpi-inf.mpg.de/~weikum/">Gerhard Weikum</a> and
                Prof. <a href="https://sites.ualberta.ca/~denilson/">Denilson Barbosa</a> at the
                <a href="https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/">
                  Max Planck Institute for Informatics</a>, and with Prof.
                  <a href="https://www.cse.iitb.ac.in/~soumen/">Soumen Chakrabarti</a>
                at <a href ="https://www.cse.iitb.ac.in/">IIT-Bombay</a>. I've also had the pleasure of working
                under the guidance of Prof. <a href="http://www.pmg.it.usyd.edu.au/">Sanjay Chawla</a> and Prof.
                <a href="https://www.cse.iitb.ac.in/~shivaram/">Shivaram Kalyanakrishnan</a> as a part of the amazing
                <a href="https://research.yahoo.com/research-areas/advertising-science">Ad-Precition team</a> at Yahoo Labs!
                I got my Bachelor's in Computer Science from <a href="http://www.bits-pilani.ac.in/">BITS - Pilani</a>, India.
              </span>
              </p>
              <p>
              <span style="font-size: 20px">

              </span>
              </p>
              </td>
            </tr>
          </tbody>
        </table>
        </div>
        <div class="well" id="misc" style="font-size: 16px">
         <h3> News </h3>
         <ul>
           <li>
          06/2017: Gave a tutorial on <a href="http://bair.berkeley.edu/blog/2017/06/20/learning-to-reason-with-neural-module-networks/">Modular Networks</a> in our Deep Learning Reading Group.
           </li>
          <li>
          05/2017: Started organizing the Deep Learning <a href="https://lists.cs.illinois.edu/lists/arc/deeplearningdiscussion">Discussion Group</a> at UIUC. Checkout our schedules (<a href="https://goo.gl/uZAdVT">Fall'17</a>, <a href="https://goo.gl/YwKVIj">Summer'17</a>) and <a href="https://goo.gl/ehJChm">reading list</a>.
          </li>
          <li>
           02/2017: Gave a tutorial on GANs in the Deep Learning Special Topics course. [<a href="http://slazebni.cs.illinois.edu/spring17/lec11_gan.pdf">PDF</a>]
          </li>
          <li>
          </li>
          <li>
          </li>
          <li>
          </li>
          </ul>
        </div>
        <div class="well" id="pubs" style="font-size: 15px">
          <h3 id="pubs">Publications</h3>
          <br/>
         <!--  <h4 id="conf">Preprints</h4>
          <ul>

          </ul>
          <br/> -->

          <h4 id="conf">Conferences</h4>
            <ul>
            <li>
              Rajarshi Das,  <a href="http://manzil.ml/">Manzil Zaheer</a>,  <a href="http://sivareddy.in/">Siva Reddy</a>, <a href="http://www.cs.umass.edu/~mccallum/">Andrew McCallum</a> "Question Answering with Universal Schema and Memory Networks." (short paper) <b>ACL 2017</b>. <span style="font-size: 14px"><a class="label label-success collapsed" data-toggle="collapse" data-target="#uschema_memnn_qa" aria-expanded="false">Abstract</a> </span><div id="uschema_memnn_qa" class="collapse" aria-expanded="false" style="font-size: 12px">
                  <p>Existing question answering methods infer answers either from a knowledge base or from raw text. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, web text contains millions of facts that are absent in the KB, however in an unstructured form. Universal schema can support reasoning on the union of both structured KBs and unstructured text by aligning them in a common embedded space. In this paper we extend universal schema to natural language question answering, employing memory networks to attend to the large body of facts in the combination of text and KB. Our models can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on SPADES fill-in-the-blank question answering dataset show that exploiting universal schema for question answering is better than using either a KB or text alone. This model also outperforms the currentstate-of-the-art by 8.5 F1 points.</p>
                </div>
                <!-- <span style="font-size: 14px"><a class="label label-warning" target="_blank" href="http://arxiv.org/pdf/1607.01426v2.pdf">PDF</a> </span>             -->
            </li>
                 <li>
                 Rajarshi Das, <a href="https://people.cs.umass.edu/~arvind/">Arvind Neelakantan</a>, <a href="https://people.cs.umass.edu/~belanger/">David Belanger</a>, <a href="http://www.cs.umass.edu/~mccallum/">Andrew McCallum</a> “Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks.” <b>EACL, 2017</b> <span style="font-size: 14px"></span>
                <span style="font-size: 14px"><a class="label label-success collapsed" data-toggle="collapse" data-target="#lstm_emnlp16" aria-expanded="false">Abstract</a> </span>
                <div id="lstm_emnlp16" class="collapse" aria-expanded="false" style="font-size: 12px">
                  <p>Our goal is to combine the rich multistep inference of symbolic logical
                    reasoning with the generalization capabilities of neural networks. We are
                    particularly interested in complex reasoning about entities and relations in
                    text and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs
                    to compose the distributed semantics of multi-hop paths in KBs; however for
                    multiple reasons, the approach lacks accuracy and practicality. This paper
                    proposes three significant modeling advances: (1) we learn to jointly reason
                    about relations, entities, and entity-types; (2) we use neural attention
                    modeling to incorporate multiple paths; (3) we learn to share strength in a
                    single RNN that represents logical composition across all relations. On a
                    largescale Freebase+ClueWeb prediction task, we achieve 25% error reduction,
                    and a 53% error reduction on sparse relations due to shared strength. On chains
                    of reasoning in WordNet we reduce error in mean quantile by 84% versus previous
                    state-of-the-art.</p>
                </div>
                <span style="font-size: 14px"><a class="label label-warning" target="_blank" href="http://arxiv.org/pdf/1607.01426v2.pdf">PDF</a> </span>
                 <span style="font-size: 14px"><a class="label label-info" target="_blank" href="talks/Chains_of_Reasoning.pdf">Slides</a> </span>
                 <span style="font-size: 14px"><a class="label label-primary" target="_blank" href="http://videolectures.net/deeplearning2016_das_neural_networks/">Talk</a> </span>
                 <span style="font-size: 14px"><a class="label label-danger" target="_blank" href="http://rajarshd.github.io/ChainsofReasoning">Code</a> </span>
            </li>
                <li>Rajarshi Das, <a href="http://manzil.ml/">Manzil Zaheer</a>, <a href="http://cs.cmu.edu/~cdyer/">Chris Dyer</a>, “Gaussian LDA for Topic Models with Word Embeddings.” <span style="font-size: 14px"><b>ACL 2015.</b></span>
                <span style="font-size: 14px"><a class="label label-success collapsed" data-toggle="collapse" data-target="#demo" aria-expanded="false">Abstract</a> </span>
                <div id="demo" class="collapse" aria-expanded="false" style="font-size: 12px">
                  <p>Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language. In this paper we replace LDA's parameterization of "topics" as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a-priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decompositions of covariance matrices of the posterior predictive distributions. We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis--Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantitatively, our technique outperforms existing models at dealing with OOV words in held-out documents. </p>
                </div>
                <span style="font-size: 14px"><a class="label label-warning" target="_blank" href="papers/acl2015.pdf">PDF</a> </span>
                <!-- <a class="label label-primary" target="_blank" href="https://github.com/rajarshd/Gaussian_LDA">Code</a> -->
                <span style="font-size: 14px"><a class="label label-primary" target="_blank" href="http://rajarshd.github.io/Gaussian_LDA/">Project Page</a></span>
                <span style="font-size: 14px"><a class="label label-info collapsed" data-toggle="collapse" data-target="#bib" aria-expanded="false">Bibtex</a></span>
                 <div id="bib" class="collapse" aria-expanded="false" style="font-size: 12px">
                 <br/>
                  @InProceedings{das-zaheer-dyer:2015:ACL-IJCNLP, <br/>
                  author    = {Das, Rajarshi  and  Zaheer, Manzil  and  Dyer, Chris}, <br/>
                  title     = {Gaussian LDA for Topic Models with Word Embeddings}, <br/>
                  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},<br/>
                  month     = {July},<br/>
                  year      = {2015},<br/>
                  address   = {Beijing, China},<br/>
                  publisher = {Association for Computational Linguistics},<br/>
                  pages     = {795--804},<br/>
                  url       = {http://www.aclweb.org/anthology/P15-1077}<br/>
                }<br/>
                 </div>
                 </li>
              <!--<li><a href="http://oro.open.ac.uk/22343/1/QG2010-Proceedings.pdf">Question Generation using Syntax and Keywords</a>. Saidalavi Kalady, Ajeesh Elikkottil, Rajarshi Das 3rd Workshop on Question Generation (QG2010) Carnegie Mellon University, Pittsburgh, Pennsylvania.</li>-->
            </ul>
          <br/>
          <h4 id="ws">Workshops</h4>
          <ul>
          <li>
            Rajarshi Das, <a href="https://people.cs.umass.edu/~arvind/">Arvind Neelakantan</a>, <a href="https://people.cs.umass.edu/~belanger/">David Belanger</a>, <a href="http://www.cs.umass.edu/~mccallum/">Andrew McCallum</a> “Incorporating Selectional Preferences in Multi-hop Relation Extraction” <span style="font-size: 14px">NAACL 2016 Workshop on Automated Knowledge Base Construction (AKBC)</span>
                <span style="font-size: 14px"><a class="label label-success collapsed" data-toggle="collapse" data-target="#demo_naacl16" aria-expanded="false">Abstract</a> </span>
                <div id="demo_naacl16" class="collapse" aria-expanded="false" style="font-size: 12px">
                  <p>Relation extraction is one of the core challenges in automated knowledge base construction. One line of approach for relation extraction is to perform multi-hop reasoning on the paths connecting an entity pair to infer new relations. While these methods have been successfully applied for knowledge base completion, they do not utilize the entity or the entity type information to make predictions. In this work, we incorporate selectional preferences, i.e., relations enforce constraints on the allowed entity types for the candidate entities, to multi-hop relation extraction by including entity type information. We achieve a 17:67% improvement in MAP score in a relation extractiontask when compared to a method that does not use entity type information. </p>
                </div>
                <span style="font-size: 14px"><a class="label label-warning" target="_blank" href="papers/naaclakbc2016.pdf">PDF</a>  </span>
                <span style="font-size: 14px"><a class="label label-primary" target="_blank" href="http://iesl.cs.umass.edu/downloads/akbc16/">Data</a> </span>
                <span style="font-size: 14px"><a class="label label-info" target="_blank" href="http://people.cs.umass.edu/~rajarshi/paths.html">Visuals!</a> </span>
          </li>
          </ul>
        </div>

<div class="well" id="contact" style="font-size: 16px">
        <h3 id="contact">Contact</h3>
          <!-- <a name="Contact"></a>  -->
          Email: sgupta96@illinois.edu
</div>

<div  id="footer">
      <span>I have used <a href="http://getbootstrap.com"> Bootstap</a> to build this.</span>
</div>

      <!-- <div id="clustrmaps-widget"></div><script type="text/javascript">var _clustrmaps = {'url' : 'http://cs.cmu.edu/~rajarshd', 'user' : 1136274, 'server' : '2', 'id' : 'clustrmaps-widget', 'version' : 1, 'date' : '2014-03-16', 'lang' : 'en', 'corners' : 'square' };(function (){ var s = document.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'http://www2.clustrmaps.com/counter/map.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x);})();</script><noscript><a href="http://www2.clustrmaps.com/user/fe7115692"><img src="http://www2.clustrmaps.com/stats/maps-no_clusters/cs.cmu.edu-~rajarshd-thumb.jpg" alt="Locations of visitors to this page" /></a></noscript> -->

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
