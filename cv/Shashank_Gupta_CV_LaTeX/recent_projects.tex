\section{\mysidestyle Recent\\ Projects}
\vspace{0mm}
\textsf{\textbf{Sparse Multi-Task Learning using Mixture-of-Experts}} \hfill\textit{\small(Sept'21 - Present)}
\\ {\textit{Mentors: Subho Mukherjee, and Ahmed Awadallah}}
\normalsize
\begin{itemize}[leftmargin=*]\compresslist
    \item[] (See Publication \#5)\vspace{-1mm}
    \item[] Introduced task-aware gating for Multi-task learning (MTL) using Mixture-of-Experts (MoE) architecture that outperformed existing dense and sparse MTL models on 3 dimensions: 1) transfer to low-resource tasks during MTL training 2) sample efficient generalization to unseen related tasks 3) robustness to catastrophic forgetting on addition of unrelated tasks. Scaling experiments demonstrated the efficacy of the approach regardless of the model scale and task diversity.
\end{itemize}

\textsf{\textbf{Knowledge infused decoding}} \hfill\textit{\small(June - Sept'21)}
\\ {\textit{Collaborators: Ruibo Liu, Guoqing Zheng, and Ahmed Awadallah}}
\normalsize
\begin{itemize}[leftmargin=*]\compresslist
\item[] (See Publication \#4)\vspace{-1mm} 
\item[] Introduced a novel decoding algorithm (KID) for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. KID outperformed task-optimized state-of-the-art models and existing knowledge-infusion techniques on six diverse knowledge-intensive NLG tasks. 
\end{itemize}

% \textsf{\textbf{Few-shot Multi-Modal Learning}} \hfill\textit{\small(July'18 - May'21)}
% \\ {\textit{Collaborators: Subho, and Ahmed}} \hfill{ \myhref[darkblue]{https://www.microsoft.com/en-us/research/group/msai/articles/assistive-ai-makes-replying-easier-2/}{Web}}
% \normalsize
% \begin{itemize}[leftmargin=*]\compresslist
% \item Developed and shipped an automated 
% \item Addressed Simple coupling of classifiers without constraints showed poor performance.
% \end{itemize}

\textsf{\textbf{Efficient model compression for Commercial Suggested Replies}} \hfill\textit{\small(Aug - Dec'20)}
\\ {\textit{Mentor: Milad Shokouhi}}
\normalsize
\begin{itemize}[leftmargin=*]\compresslist
\item[] (See Publication \#3)\vspace{-1mm}
\item[] Explored several low-cost model compression and domain adaptation techniques for PLMs, and successfully reduced the training and inference times of a commercial email reply suggestion system by 42\% and 35\% respectively.
\end{itemize}

\textsf{\textbf{Automated Suggested Replies}} \hfill\textit{\small(July'18 - May'21)}
\\ {\textit{Mentor: Milad Shokouhi}} \hfill{ \myhref[darkblue]{https://www.microsoft.com/en-us/research/group/msai/articles/assistive-ai-makes-replying-easier-2/}{Web}}
\normalsize
\begin{itemize}[leftmargin=*]\compresslist
\item[] Developed and shipped a PLM-based email reply suggestion feature for millions of Outlook and Teams users. Project involved creating pipelines for response candidate generation, large-scale training of a transformer-based matching model on millions of examples, and addressing gender-bias and response diversity issues.
\end{itemize}