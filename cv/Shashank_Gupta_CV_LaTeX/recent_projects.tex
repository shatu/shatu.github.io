\section{\mysidestyle Recent\\ Projects}
\vspace{0mm}
\textsf{\textbf{Sparse Multi-Task Learning using Mixture-of-Experts}} \hfill\textit{\small(Sept'21 - Present)}
\\ {\textit{Mentors: \href{https://www.microsoft.com/en-us/research/people/submukhe/}{Subho Mukherjee}, and \href{https://www.microsoft.com/en-us/research/people/hassanam/}{Ahmed Awadallah}; MSR Redmond}}
\normalsize
\begin{itemize}[leftmargin=*]\compresslist
    \item[] (See Publication \#5)\vspace{-1mm}
    \item[] Introduced task-aware gating in Mixture-of-Experts architectures for Multi-task learning (MTL) that outperformed existing dense and sparse MTL models on three key dimensions: 1) transfer to low-resource tasks during MTL training. 2) sample-efficient generalization to unseen related tasks. 3) robustness to catastrophic forgetting on the addition of unrelated tasks. Scaling experiments demonstrated the efficacy of the approach regardless of the model scale and task diversity.
\end{itemize}

\textsf{\textbf{Knowledge Infused Decoding}} \hfill\textit{\small(June - Sept'21)}
\\ {\textit{Mentor: \href{https://www.microsoft.com/en-us/research/people/hassanam/}{Ahmed Awadallah}; MSR Redmond}}
\normalsize
\begin{itemize}[leftmargin=*]\compresslist
\item[] (See Publication \#4)\vspace{-1mm} 
\item[] Introduced a novel decoding algorithm (KID) for generative LMs that dynamically infuses external knowledge into each step of the LM decoding. KID outperformed task-optimized state-of-the-art models and existing knowledge-infusion techniques on six diverse knowledge-intensive NLG tasks. 
\end{itemize}

% \textsf{\textbf{Few-shot Multi-Modal Learning}} \hfill\textit{\small(July'18 - May'21)}
% \\ {\textit{Collaborators: Subho, and Ahmed}} \hfill{ \myhref[darkblue]{https://www.microsoft.com/en-us/research/group/msai/articles/assistive-ai-makes-replying-easier-2/}{Web}}
% \normalsize
% \begin{itemize}[leftmargin=*]\compresslist
% \item Developed and shipped an automated 
% \item Addressed Simple coupling of classifiers without constraints showed poor performance.
% \end{itemize}

\textsf{\textbf{Efficient model compression for Commercial Suggested Replies}} \hfill\textit{\small(Aug - Dec'20)}
\\ {\textit{Mentor: \href{https://www.microsoft.com/en-us/research/people/milads/}{Milad Shokouhi}; Microsoft AI}}
\normalsize
\begin{itemize}[leftmargin=*]\compresslist
\item[] (See Publication \#3)\vspace{-1mm}
\item[] Explored several low-cost model compression techniques for PLMs to successfully reduce the training and inference times of a commercial email reply suggestion system by 42\% and 35\% respectively. Studied the efficacy of compression techniques with variations in the dataset size and PLM quality and obtained some key recommendations for industrial applications.
\end{itemize}

\textsf{\textbf{Automated Suggested Replies}} \hfill\textit{\small(July'18 - June'21)}
\\ {\textit{Mentor: \href{https://www.microsoft.com/en-us/research/people/milads/}{Milad Shokouhi}; Microsoft AI}} \hfill{ \myhref[darkblue]{https://www.microsoft.com/en-us/research/group/msai/articles/assistive-ai-makes-replying-easier-2/}{Web}}
\normalsize
\begin{itemize}[leftmargin=*]\compresslist
\item[] Developed and productionized a PLM-based automated reply suggestion feature for millions of Outlook and Teams users. The project involved developing pipelines for generating candidate responses, modeling it as an information retrieval task with large-scale training of a transformer-based matching model on millions of users, identifying and addressing gender bias and response diversity issues, and carrying out principled offline and A/B experiments to measure the impact on user engagement.
\end{itemize}